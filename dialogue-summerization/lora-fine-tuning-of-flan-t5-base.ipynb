{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Fine-tune [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) for Dualogue Summarization","metadata":{"_uuid":"b71f61f5-979f-44e1-b327-5a109754b72e","_cell_guid":"ae17a6e6-03bb-49d0-8a7d-529bf3d175c9","trusted":true}},{"cell_type":"markdown","source":"In this notebook, [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) will be fine-tuned on [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. It contains 10,000+ dialogues with corresponding manually labeled summeries","metadata":{"_uuid":"834bdf8a-f590-403f-91c5-fd48da8c8422","_cell_guid":"1764c420-bd82-4e42-b30b-0d3702350a3a","trusted":true}},{"cell_type":"code","source":"# install Hugging Face Libraries\n!pip install \"peft==0.2.0\"\n!pip install \"transformers==4.27.2\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes==0.37.1\" loralib --upgrade --quiet\n# install additional dependencies needed for training\n!pip install rouge-score tensorboard py7zr","metadata":{"execution":{"iopub.status.busy":"2023-10-19T21:49:58.087071Z","iopub.execute_input":"2023-10-19T21:49:58.087403Z","iopub.status.idle":"2023-10-19T21:50:39.802714Z","shell.execute_reply.started":"2023-10-19T21:49:58.087375Z","shell.execute_reply":"2023-10-19T21:50:39.801664Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting peft==0.2.0\n  Downloading peft-0.2.0-py3-none-any.whl (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0) (4.33.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0) (0.22.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.2.0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.2.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.2.0) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.2.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.2.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.2.0) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.2.0) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.2.0) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.2.0) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.2.0) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.2.0) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.2.0) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers->peft==0.2.0) (2023.9.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.2.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.2.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.2.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.2.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.2.0) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.2.0) (1.3.0)\nInstalling collected packages: peft\nSuccessfully installed peft-0.2.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\npathos 0.3.1 requires dill>=0.3.7, but you have dill 0.3.6 which is incompatible.\npathos 0.3.1 requires multiprocess>=0.70.15, but you have multiprocess 0.70.14 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.12.3)\nCollecting py7zr\n  Downloading py7zr-0.20.6-py3-none-any.whl (66 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.4.3)\nRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (68.0.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.3.7)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.40.0)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.6.7)\nCollecting pycryptodomex>=3.6.6 (from py7zr)\n  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pyzstd>=0.14.4 (from py7zr)\n  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyppmd<1.1.0,>=0.18.1 (from py7zr)\n  Downloading pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pybcj>=0.6.0 (from py7zr)\n  Downloading pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr)\n  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nCollecting brotli>=1.0.9 (from py7zr)\n  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting inflate64>=0.3.1 (from py7zr)\n  Downloading inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from py7zr) (5.9.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=22df2f48ea5fcfd35d0ecfa0ede2f5320ee3c3199334fdd19860fde0e3a7a66a\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, rouge-score, py7zr\nSuccessfully installed brotli-1.1.0 inflate64-0.3.1 multivolumefile-0.2.3 py7zr-0.20.6 pybcj-1.0.1 pycryptodomex-3.19.0 pyppmd-1.0.0 pyzstd-0.15.9 rouge-score-0.1.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load the dataset","metadata":{"_uuid":"4ca011c6-2b64-407c-ad3b-714c7958c392","_cell_guid":"ba23490a-d45e-405c-9271-ae112876222b","trusted":true}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load dataset from the hub\ndataset = load_dataset(\"samsum\")\n\nprint(f\"Train dataset size: {len(dataset['train'])}\")\nprint(f\"Test dataset size: {len(dataset['test'])}\")\n\n# Train dataset size: 14732\n# Test dataset size: 819","metadata":{"_uuid":"7df02c6c-5d59-4770-a463-16fd5333833a","_cell_guid":"79898980-88d0-4898-82db-aa72888b3aef","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T21:50:39.804966Z","iopub.execute_input":"2023-10-19T21:50:39.805317Z","iopub.status.idle":"2023-10-19T21:50:44.362892Z","shell.execute_reply.started":"2023-10-19T21:50:39.805286Z","shell.execute_reply":"2023-10-19T21:50:44.361981Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/3.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b911f93600647be8dadd9537b56ca68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4b4290167884c3a9abab5b8fbb775c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/7.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec5ba28fa3324827a413acaf872f10ec"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Downloading and preparing dataset samsum/samsum to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"051ecfac24a146969ac760383a37b196"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset samsum downloaded and prepared to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93067bdb1e9a43f4b171419ec5259a64"}},"metadata":{}},{"name":"stdout","text":"Train dataset size: 14732\nTest dataset size: 819\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load the [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) model and it's Tokenizer","metadata":{"_uuid":"8881b4bf-2b42-4dae-b0bf-6fd7733693cd","_cell_guid":"d998e1cb-7e9f-4c81-8f68-764714a93c37","trusted":true}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\nmodel_id=\"google/flan-t5-base\"\n\n# Load tokenizer of FLAN-t5-XL\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"_uuid":"29f4ac27-fab0-454d-bb96-2a6166d2455e","_cell_guid":"3683854f-5c0f-4d0e-b097-01d0b37ee335","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T21:50:44.364152Z","iopub.execute_input":"2023-10-19T21:50:44.364486Z","iopub.status.idle":"2023-10-19T21:50:47.984753Z","shell.execute_reply.started":"2023-10-19T21:50:44.364456Z","shell.execute_reply":"2023-10-19T21:50:47.983781Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab5d7050f5204c4fb099dfe69013773e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f0e1cdbcdb54007b0e82d2cd59a8b5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1065dc3cfe154d788552c1896f8124bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f58d3ddbd354f26b55e2e0cadae83d2"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Fine-Tune T5 with LoRA and bnb int-8","metadata":{}},{"cell_type":"code","source":"model_id=\"google/flan-t5-base\"\n# load model from the hub\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")","metadata":{"execution":{"iopub.status.busy":"2023-10-19T21:50:47.986605Z","iopub.execute_input":"2023-10-19T21:50:47.987069Z","iopub.status.idle":"2023-10-19T21:51:05.309614Z","shell.execute_reply.started":"2023-10-19T21:50:47.987045Z","shell.execute_reply":"2023-10-19T21:51:05.308965Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c40668447c604d2a90d2a3b86262d21d"}},"metadata":{}},{"name":"stderr","text":"Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7b79773180142289659b15549e9e0f7"}},"metadata":{}},{"name":"stdout","text":"\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 6.0\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n  warn(msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4b8d232e4574e739893d3fc7a8b1fa3"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Test Zero-shot Inference","metadata":{}},{"cell_type":"code","source":"index = 200\n\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n\"\"\"\n\ninputs = tokenizer(prompt, return_tensors='pt').to('cuda')\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"]\n    )[0], \n    skip_special_tokens=True\n)\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"execution":{"iopub.status.busy":"2023-10-19T21:51:05.310481Z","iopub.execute_input":"2023-10-19T21:51:05.310694Z","iopub.status.idle":"2023-10-19T21:51:09.549058Z","shell.execute_reply.started":"2023-10-19T21:51:05.310675Z","shell.execute_reply":"2023-10-19T21:51:09.548231Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nSummarize the following conversation.\n\nAbdellilah: Where are you?\nSam: work\nAbdellilah: What time you finish?\nSam: Not til 5\nAbdellilah: Are your bringing him over tonight:\nSam: No in the morning:\nAbdellilah: ok, what time?\nSam: About 9. Is that ok?\nAbdellilah: ok - see you then\n\nSummary:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\nSam won't finish work till 5. Sam is bringing him over about 9 am. Sam will see Abdellilah in the morning. \n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nSam finishes work at 5 and will bring Abdellilah to work at about 9.\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset['test'][200]","metadata":{"execution":{"iopub.status.busy":"2023-10-19T21:51:09.550093Z","iopub.execute_input":"2023-10-19T21:51:09.550417Z","iopub.status.idle":"2023-10-19T21:51:09.557288Z","shell.execute_reply.started":"2023-10-19T21:51:09.550387Z","shell.execute_reply":"2023-10-19T21:51:09.556471Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'id': '13611693',\n 'dialogue': 'Abdellilah: Where are you?\\r\\nSam: work\\r\\nAbdellilah: What time you finish?\\r\\nSam: Not til 5\\r\\nAbdellilah: Are your bringing him over tonight:\\r\\nSam: No in the morning:\\r\\nAbdellilah: ok, what time?\\r\\nSam: About 9. Is that ok?\\r\\nAbdellilah: ok - see you then',\n 'summary': \"Sam won't finish work till 5. Sam is bringing him over about 9 am. Sam will see Abdellilah in the morning. \"}"},"metadata":{}}]},{"cell_type":"markdown","source":"<a name='2.1'></a>\n### Preprocess the Dialog-Summary Dataset\n\nYou need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with `Summarize the following conversation` and to the start of the summary with `Summary` as follows:\n\nTraining prompt (dialogue):\n```\nSummarize the following conversation.\n\n    Chris: This is his part of the conversation.\n    Antje: This is her part of the conversation.\n    \nSummary: \n```\n\nTraining response (summary):\n```\nBoth Chris and Antje participated in the conversation.\n```\n\nThen preprocess the prompt-response dataset into tokens and pull out their `input_ids` (1 per token).","metadata":{"_uuid":"900877e0-f42f-4a65-8bdb-a7dcf50e39a9","_cell_guid":"e8679b87-3d3c-44b3-a97f-e552bebfb183","trusted":true}},{"cell_type":"code","source":"def tokenize_function(example):\n    start_prompt = 'Summarize the following conversation.\\n\\n'\n    end_prompt = '\\n\\nSummary: '\n    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids.to('cuda')\n    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids.to('cuda')\n    \n    return example\n\n# The dataset actually contains 3 diff splits: train, validation, test.\n# The tokenize_function code is handling all data across all splits in batches.\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns(['id', 'dialogue', 'summary',])","metadata":{"execution":{"iopub.status.busy":"2023-10-19T21:51:09.558636Z","iopub.execute_input":"2023-10-19T21:51:09.558984Z","iopub.status.idle":"2023-10-19T21:51:21.435397Z","shell.execute_reply.started":"2023-10-19T21:51:09.558950Z","shell.execute_reply":"2023-10-19T21:51:21.434608Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67b44a3f30ec449a8fc75f1e78e7f1ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8e1188d3f844473b500c8066fc82a91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"015a6f1bd8d74bfe90e5c66e42d885b1"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2023-10-20T02:36:00.252625Z","iopub.execute_input":"2023-10-20T02:36:00.253244Z","iopub.status.idle":"2023-10-20T02:36:00.259001Z","shell.execute_reply.started":"2023-10-20T02:36:00.253214Z","shell.execute_reply":"2023-10-20T02:36:00.258165Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 818\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets['train'][200]","metadata":{"execution":{"iopub.status.busy":"2023-10-19T21:51:21.436605Z","iopub.execute_input":"2023-10-19T21:51:21.436973Z","iopub.status.idle":"2023-10-19T21:51:21.453289Z","shell.execute_reply.started":"2023-10-19T21:51:21.436941Z","shell.execute_reply":"2023-10-19T21:51:21.452157Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [12198,\n  1635,\n  1737,\n  8,\n  826,\n  3634,\n  5,\n  15868,\n  10,\n  454,\n  13747,\n  476,\n  8747,\n  3,\n  476,\n  427,\n  188,\n  5498,\n  5498,\n  4280,\n  1603,\n  3,\n  2,\n  599,\n  2,\n  32,\n  2,\n  61,\n  2,\n  15868,\n  10,\n  45,\n  140,\n  3,\n  184,\n  3059,\n  3,\n  10,\n  61,\n  4496,\n  10,\n  1333,\n  1603,\n  31714,\n  10,\n  337,\n  12,\n  25,\n  31714,\n  10,\n  3,\n  10,\n  61,\n  15868,\n  10,\n  932,\n  25,\n  129,\n  1776,\n  125,\n  25,\n  33,\n  11873,\n  16,\n  8,\n  1107,\n  215,\n  3158,\n  31714,\n  10,\n  3,\n  10,\n  1935,\n  4496,\n  10,\n  571,\n  31,\n  7,\n  39,\n  1088,\n  58,\n  15868,\n  10,\n  3,\n  6210,\n  3,\n  6210,\n  3,\n  6210,\n  4496,\n  10,\n  1626,\n  1024,\n  1024,\n  107,\n  4496,\n  10,\n  27,\n  217,\n  15868,\n  10,\n  8,\n  200,\n  1088,\n  3,\n  15,\n  162,\n  15,\n  15,\n  15,\n  15,\n  15,\n  15,\n  49,\n  15868,\n  10,\n  1663,\n  25,\n  130,\n  270,\n  3158,\n  20698,\n  10,\n  3,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'labels': [15868,\n  9543,\n  3,\n  9,\n  1095,\n  126,\n  215,\n  45,\n  160,\n  11,\n  3059,\n  12,\n  31714,\n  11,\n  4496,\n  5,\n  15868,\n  19,\n  578,\n  8,\n  200,\n  97,\n  68,\n  3041,\n  15,\n  7,\n  31714,\n  11,\n  4496,\n  5,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0]}"},"metadata":{}}]},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {tokenized_datasets['train'].shape}\")\nprint(f\"Validation: {tokenized_datasets['validation'].shape}\")\nprint(f\"Test: {tokenized_datasets['test'].shape}\")\n\nprint(tokenized_datasets)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T21:51:21.454492Z","iopub.execute_input":"2023-10-19T21:51:21.454771Z","iopub.status.idle":"2023-10-19T21:51:21.475855Z","shell.execute_reply.started":"2023-10-19T21:51:21.454750Z","shell.execute_reply":"2023-10-19T21:51:21.474965Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Shapes of the datasets:\nTraining: (14732, 2)\nValidation: (818, 2)\nTest: (819, 2)\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 818\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n\n# Define LoRA Config\nlora_config = LoraConfig(\n r=16,\n lora_alpha=32,\n target_modules=[\"q\", \"v\"],\n lora_dropout=0.05,\n bias=\"none\",\n task_type=TaskType.SEQ_2_SEQ_LM\n)\n# prepare int-8 model for training\nmodel = prepare_model_for_int8_training(model)\n\n# add LoRA adaptor\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2023-10-19T21:51:21.478676Z","iopub.execute_input":"2023-10-19T21:51:21.478939Z","iopub.status.idle":"2023-10-19T21:51:21.563498Z","shell.execute_reply.started":"2023-10-19T21:51:21.478899Z","shell.execute_reply":"2023-10-19T21:51:21.562846Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"trainable params: 1769472 || all params: 249347328 || trainable%: 0.7096414524241463\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Let's Perform the Full Fine-Tuning","metadata":{"_uuid":"abc50c0b-e750-4217-892a-27ab38fdec15","_cell_guid":"9e96433c-e6ef-4dcb-ace8-7e86adbeeffe","trusted":true}},{"cell_type":"markdown","source":"<a name='3'></a>\n## Perform Parameter Efficient Fine-Tuning (PEFT)\n\nNow, let's perform **Parameter Efficient Fine-Tuning (PEFT)** fine-tuning as opposed to \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning - with comparable evaluation results as you will see soon. \n\nPEFT is a generic term that includes **Low-Rank Adaptation (LoRA)** and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU). After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged and a newly-trained “LoRA adapter” emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  \n\nThat said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request.  The benefit, however, is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases.","metadata":{"_uuid":"e86f14c3-8c18-41f0-9544-1e908ec92a52","_cell_guid":"31f9353b-054a-46e0-a48e-6d5401096e99","trusted":true}},{"cell_type":"markdown","source":"<a name='3.1'></a>\n### Setup the PEFT/LoRA model for Fine-Tuning\n\nYou need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (`r`) hyper-parameter, which defines the rank/dimension of the adapter to be trained.","metadata":{"_uuid":"83257d5d-ef27-480f-9143-192aaa0d2d79","_cell_guid":"5074ed4b-8318-4b20-8823-f7ca2c804313","trusted":true}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\n# we want to ignore tokenizer pad token in the loss\nlabel_pad_token_id = -100\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=label_pad_token_id,\n    pad_to_multiple_of=8\n)","metadata":{"_uuid":"8fc9ca5d-3c0f-4e7c-b571-a92aa9477d38","_cell_guid":"4969b1f7-8f35-4756-b67a-1418801a3a69","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T21:51:21.564253Z","iopub.execute_input":"2023-10-19T21:51:21.564452Z","iopub.status.idle":"2023-10-19T21:51:21.886936Z","shell.execute_reply.started":"2023-10-19T21:51:21.564434Z","shell.execute_reply":"2023-10-19T21:51:21.886247Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\noutput_dir=\"lora-flan-t5\"\n\n# Define training args\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    predict_with_generate=True,\n    fp16=False, # Overflows with fp16\n    learning_rate=1e-5,\n    num_train_epochs=5,\n    # logging & evaluation strategies\n    logging_dir=f\"{output_dir}/logs\",\n    logging_strategy=\"steps\",\n    logging_steps=100,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    # metric_for_best_model=\"overall_f1\",\n    # push to hub parameters\n    report_to=\"tensorboard\",\n)\n\n# Create Trainer instance\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T21:51:21.888223Z","iopub.execute_input":"2023-10-19T21:51:21.888601Z","iopub.status.idle":"2023-10-19T21:51:21.924116Z","shell.execute_reply.started":"2023-10-19T21:51:21.888566Z","shell.execute_reply":"2023-10-19T21:51:21.923584Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-10-19T21:51:21.925234Z","iopub.execute_input":"2023-10-19T21:51:21.925539Z","iopub.status.idle":"2023-10-20T02:23:39.033887Z","shell.execute_reply.started":"2023-10-19T21:51:21.925510Z","shell.execute_reply":"2023-10-20T02:23:39.032984Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9210' max='9210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9210/9210 4:32:15, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.779300</td>\n      <td>1.038203</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.470700</td>\n      <td>0.271904</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.332200</td>\n      <td>0.186550</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.264200</td>\n      <td>0.162611</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.264900</td>\n      <td>0.154865</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=9210, training_loss=2.8007446545860795, metrics={'train_runtime': 16337.083, 'train_samples_per_second': 4.509, 'train_steps_per_second': 0.564, 'total_flos': 5.083962893402112e+16, 'train_loss': 2.8007446545860795, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Save the fine-tuned model","metadata":{}},{"cell_type":"code","source":"# Save our LoRA model & tokenizer results\npeft_model_id=\"lora-flan-t5\"\ntrainer.model.save_pretrained(peft_model_id)\ntokenizer.save_pretrained(peft_model_id)\n# if you want to save the base model to call\n# trainer.model.base_model.save_pretrained(peft_model_id)","metadata":{"execution":{"iopub.status.busy":"2023-10-20T02:23:39.034949Z","iopub.execute_input":"2023-10-20T02:23:39.035218Z","iopub.status.idle":"2023-10-20T02:23:39.110680Z","shell.execute_reply.started":"2023-10-20T02:23:39.035196Z","shell.execute_reply":"2023-10-20T02:23:39.109989Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('lora-flan-t5/tokenizer_config.json',\n 'lora-flan-t5/special_tokens_map.json',\n 'lora-flan-t5/spiece.model',\n 'lora-flan-t5/added_tokens.json',\n 'lora-flan-t5/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluate & run Inference with LoRA FLAN-T5","metadata":{}},{"cell_type":"code","source":"import torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# Load peft config for pre-trained checkpoint etc.\npeft_model_id = \"lora-flan-t5\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\n\nbase_model_id=\"google/flan-t5-base\"\n\n# load base LLM model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(base_model_id,  load_in_8bit=True,  device_map={\"\":0})\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\n# Load the Lora model\nmodel = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\nmodel.eval()\n\nprint(\"Peft model loaded\")","metadata":{"execution":{"iopub.status.busy":"2023-10-20T02:23:39.111587Z","iopub.execute_input":"2023-10-20T02:23:39.111809Z","iopub.status.idle":"2023-10-20T02:23:41.247740Z","shell.execute_reply.started":"2023-10-20T02:23:39.111790Z","shell.execute_reply":"2023-10-20T02:23:41.246936Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n","output_type":"stream"},{"name":"stdout","text":"Peft model loaded\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Let’s load the dataset again with a random sample to try the summarization.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom random import randrange\n\n\n# Load dataset from the hub and get a sample\ndataset = load_dataset(\"samsum\")\nsample = dataset['test'][randrange(len(dataset[\"test\"]))]\n\ninput_ids = tokenizer(sample[\"dialogue\"], return_tensors=\"pt\", truncation=True).input_ids.cuda()\n# with torch.inference_mode():\noutputs = model.generate(input_ids=input_ids, max_new_tokens=10, do_sample=True, top_p=0.9)\nprint(f\"input sentence: {sample['dialogue']}\\n{'---'* 20}\")\n\nprint(f\"summary:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-20T02:23:41.248692Z","iopub.execute_input":"2023-10-20T02:23:41.248979Z","iopub.status.idle":"2023-10-20T02:23:42.930019Z","shell.execute_reply.started":"2023-10-20T02:23:41.248952Z","shell.execute_reply":"2023-10-20T02:23:42.929143Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6daaefbc9248417697ab1168098362a7"}},"metadata":{}},{"name":"stdout","text":"input sentence: Richie: Pogba\nClay: Pogboom\nRichie: what a s strike yoh!\nClay: was off the seat the moment he chopped the ball back to his right foot\nRichie: me too dude\nClay: hope his form lasts\nRichie: This season he's more mature\nClay: Yeah, Jose has his trust in him\nRichie: everyone does\nClay: yeah, he really deserved to score after his first 60 minutes\nRichie: reward\nClay: yeah man\nRichie: cool then \nClay: cool\n------------------------------------------------------------\nsummary:\nPogba scored a strike after his\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Evaluate it against the test set of processed dataset from samsum","metadata":{}},{"cell_type":"code","source":"import evaluate\nimport numpy as np\nfrom datasets import load_from_disk\nfrom tqdm import tqdm\n\n# Metric\nmetric = evaluate.load(\"rouge\")\n\ndef evaluate_peft_model(sample,max_target_length=50):\n    # generate summary\n    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)\n    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n    # decode eval sample\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n    labels = tokenizer.decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    return prediction, labels\n\n# load test dataset from distk\ntest_dataset = tokenized_datasets['test'].with_format(\"torch\")\n\n# run predictions\n# this can take ~45 minutes\npredictions, references = [] , []\nfor sample in tqdm(test_dataset):\n    p,l = evaluate_peft_model(sample)\n    predictions.append(p)\n    references.append(l)\n\n# compute metric\nrogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n\n# print results\nprint(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\nprint(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\nprint(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\nprint(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")\n\n# Rogue1: 44.588333%\n# rouge2: 19.431970%\n# rougeL: 35.811457%\n# rougeLsum: 35.794847%","metadata":{"execution":{"iopub.status.busy":"2023-10-20T02:38:24.870120Z","iopub.execute_input":"2023-10-20T02:38:24.870931Z","iopub.status.idle":"2023-10-20T03:07:42.896721Z","shell.execute_reply.started":"2023-10-20T02:38:24.870886Z","shell.execute_reply":"2023-10-20T03:07:42.895570Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"100%|██████████| 819/819 [29:16<00:00,  2.14s/it]\n","output_type":"stream"},{"name":"stdout","text":"Rogue1: 44.588333%\nrouge2: 19.431970%\nrougeL: 35.811457%\nrougeLsum: 35.794847%\n","output_type":"stream"}]}]}